{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-18T02:20:01.161712Z","iopub.status.busy":"2024-08-18T02:20:01.161322Z"},"trusted":true},"outputs":[],"source":["!git clone https://github.com/ahmed-fz11/LLM-Nav.git"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%cd LLM-Nav"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -U bitsandbytes\n","!pip install --upgrade transformers\n","!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from pydantic import BaseModel\n","from openai import OpenAI\n","import pandas as pd\n","import os\n","import glob\n","\n","from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n","import torch\n","from PIL import Image\n","\n","CLIENT = OpenAI(api_key=\"add api key here\")\n","\n","def summarize_experiences(past_summary,hlp):\n","    input_text = f\"Past Summary: {past_summary}\\nNew plan: {hlp}\"\n","    try:\n","        response = CLIENT.chat.completions.create(\n","            model='gpt-3.5-turbo',\n","             messages=[\n","                {\"role\": \"system\", \"content\": \n","                 \"\"\"You are a summarizer which gives an answer in past tense.Given a past summary and and recent actions you must combine this into a single summary of past experiences. Give your answer in past tense, telling me what I have done till now.\n","                Use the format:\n","                Summary:\"\"\"},  # Optional system message\n","                {\"role\": \"user\", \"content\": input_text}\n","            ],\n","            temperature=0,\n","            max_tokens=100,\n","            top_p=1,\n","            frequency_penalty=0.0,\n","            presence_penalty=0.0,\n","        )\n","        # Output the GPT response\n","        output_text = response.choices[0].message.content\n","        return output_text.split('Summary:')[1].strip()\n","    except Exception as e:\n","        print(f\"Error with GPT API request: {e}\")\n","        return None\n","\n","\n","def Llava_generation(input_text,image_names,base_path):\n","    processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n","    model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True, load_in_4bit=True)\n","    # model.to(0) # 4bit and 8bit quantization automatically puts model on GPU\n","\n","    # prepare image and text prompt, using the appropriate prompt template\n","    image_names.sort()\n","    images = [Image.open(os.path.join(base_path, image_name)) for image_name in image_names]\n","\n","    # Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt\n","    # Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n","    conversation = [\n","    {\n","\n","      \"role\": \"user\",\n","      \"content\": [\n","          {\"type\": \"text\", \"text\": input_text}, \n","        ]+[{\"type\": \"image\"} for img in images],\n","    },\n","    ]\n","\n","    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n","    print(\"prompt = \",prompt)\n","    inputs = processor(prompt, images=images, return_tensors=\"pt\").to(0)\n","\n","    # autoregressively complete prompt\n","    output = model.generate(**inputs,max_new_tokens=1000)\n","    \n","    result = processor.decode(output[0], skip_special_tokens=True).split(\"[/INST]\")[1]\n","    return result.strip()\n","\n","\n","# Only checking for first training case\n","Goal = \"I want to go to the hallway next to the kitchen.\"\n","r2r_dataset = pd.read_json(\"Dataset/R2R_train.json\")\n","\n","r2r_dataset = r2r_dataset[r2r_dataset['scan'] == \"17DRP5sb8fy\"]\n","\n","img_folder_path = 'Dataset/17DRP5sb8fy/matterport_color_images'\n","\n","\n","for index, row in r2r_dataset.iterrows():\n","    # Get relevant images for the data Point\n","    paths = row['path']\n","\n","    generated_plan = []\n","    summary = \"This is the first time, just summarize the new high level plan\"\n","\n","\n","    for path in paths:\n","        prompt = f\"\"\"My Goal is :{Goal}. Use the images provided to get a scene understanding .tell me what the next step I need to take to navigate to my goal, in one sentence\"\"\"\n","\n","        start_word = path\n","        pattern = os.path.join(img_folder_path, f'{start_word}*')\n","        image_files = glob.glob(pattern)\n","        image_names = [os.path.basename(image) for image in image_files]\n","\n","        hlp = Llava_generation(prompt,image_names,img_folder_path)\n","\n","        if(hlp == \"end\"):\n","            break\n","\n","        generated_plan.append(hlp)\n","        summary = summarize_experiences(summary,hlp)\n","\n","        break\n","\n","    print(\"final plan = \",generated_plan)\n","    print(\"final Summary = \",summary)\n","\n","    # TODO:\n","    # Check generated_plan with r2r_dataset.iloc[index]['instructions']\n","    # Compute Similarty metrics\n","\n","    break"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
